# -*- coding: utf-8 -*-
"""MiamiHousingSalesPricePredictor.ipynb

Automatically generated by Colab

Data Loading and Initial Exploration
"""

# Import necessary libraries
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import PolynomialFeatures
import warnings
warnings.filterwarnings('ignore')


# Set display options
pd.set_option('display.max_columns', None)
plt.style.use('default')

"""Load the Dataset"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("deepcontractor/miami-housing-dataset")

print("Path to dataset files:", path)

"""Create a Dataframe"""

# Check what files were downloaded
files = os.listdir(path)
print("Downloaded files:", files)

# Load the correct CSV file into a DataFrame

csv_file = [file for file in files if file.endswith('.csv')][0]  # Get first .csv file
df = pd.read_csv(os.path.join(path, csv_file))

# Display shape of the DataFrame
print(f"Dataset Shape: {df.shape}")

"""Show first 5 rows"""

# Shows first 5 Rows
print(df.head())

"""Basic Information about the dataset"""

print('Dataset Info:')
df.info()

"""Generate Descriptive stats"""

df.describe()

"""Check for Missing Values"""

print('Missing values per column:')
missing_values = df.isnull().sum()
print(missing_values)

# finding the total missing values
print(f'Total Missing Values: {missing_values.sum()}')
# find the percentage of missing data
print(f'Percentage of Missing Values: {(missing_values.sum() /len(df)) * 100:.2f}%')

"""Exploratory Data and Analysis"""

# Target Variable Distribution
plt.figure(figsize=(12, 4))

# Histogram
plt.subplot(1, 2, 1)
plt.hist(df['SALE_PRC'], bins=50, edgecolor='black', alpha=0.7)
plt.title('Distribution of Housing Prices')
plt.xlabel('Price (USD)')
plt.ylabel('Frequency')

# Box Plot
plt.subplot(1, 2, 2)
plt.boxplot(df['SALE_PRC'])
plt.title('Box Plot of Housing Prices')
plt.ylabel('Price (USD)')

plt.tight_layout()
plt.show()

"""Price Statistics"""

print('Price Statistics')
print(f"Mean: ${df['SALE_PRC'].mean():.2f} (hundreds of thousands)")
print(f"Median: ${df['SALE_PRC'].median():.2f} (hundreds of thousands)")
print(f"Standard Deviation: ${df['SALE_PRC'].std():.2f} (hundreds of thousands)")

"""Correlation Matrix"""

plt.figure(figsize=(16, 11))

correlation_matrix = df.corr()

# Heatmap with max 3 decimals
sns.heatmap(correlation_matrix, annot=True, fmt=".3f", cmap='coolwarm', center=0,
            square=True, linewidths=0.5)

plt.title('Correlation Matrix of Housing Features')
plt.show()

# Show correlations with the target variable
print('Correlations with Price (sorted bya absolute value)')
price_correlations = correlation_matrix['SALE_PRC'].drop('SALE_PRC').sort_values(key = abs, ascending=False)
price_correlations

"""Scatter Plot of Most Correlated Feautures"""

fig, axes = plt.subplots(2, 2, figsize = (15, 12))
fig.suptitle('Relationships Between Key Features and Housing Price')

# Get top 4 most correlated features (by absolute value)
top_features = price_correlations.head(4).index

for i, feature in enumerate(top_features):
  row = i // 2
  col = i % 2

  axes[row, col].scatter(df[feature], df['SALE_PRC'], alpha = 0.5)
  axes[row, col].set_xlabel(feature)
  axes[row, col].set_ylabel('SALE_PRC')
  axes[row, col].set_title(f'{feature} vs Price r ={price_correlations[feature]:.3f})')

  # Add trend line
  z = np.polyfit(df[feature], df['SALE_PRC'], 1)
  p = np.poly1d(z)
  axes[row, col].plot(df[feature], p(df[feature]), 'r--', alpha=0.8)

plt.tight_layout()
plt.show()

"""Seperate Target from Features"""

# Target is Sale Price
X = df.drop('SALE_PRC', axis= 1)
y = df['SALE_PRC']

# Get Feature Names
print(f'Feature names: {list(X.columns)}')

# Split into testing and training sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# take a look at the shapes of the training and testing sets
print(f'\n Training set shape {X_train.shape}')
print(f'Test set shape {X_test.shape}')
print(f'Training target set shape {y_train.shape}')
print(f'Testing target set shape {y_test.shape}')

"""Feature Scaling"""

# Feature scaling using StandardScaler
print('Before scaling - Training set statistics')
print(X_train.describe())

# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

type(X_train_scaled)

# Convert back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns, index = X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_test.columns, index = X_test.index)

print('After Scaler - Training set statistics')
print(X_train_scaled.describe())

"""Linear Regression & Random Forrest Model"""

# Linear Regression Model
linear_model = LinearRegression()
linear_model.fit(X_train_scaled, y_train)
y_pred_linear = linear_model.predict(X_test_scaled)

# Random Forest Regressor Model
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_leaf=5,
    random_state=42
)
rf_model.fit(X_train_scaled, y_train)

import joblib
joblib.dump(rf_model, "random_forest_model.pkl", compress=3)


y_pred_rf = rf_model.predict(X_test_scaled)

# Define the evaluation function for both training and testing
def evaluate_model(y_train_true, y_train_pred, y_test_true, y_test_pred, model_name):
    # Training metrics
    mse_train = mean_squared_error(y_train_true, y_train_pred)
    rmse_train = np.sqrt(mse_train)
    mae_train = mean_absolute_error(y_train_true, y_train_pred)
    r2_train = r2_score(y_train_true, y_train_pred)

    # Testing metrics
    mse_test = mean_squared_error(y_test_true, y_test_pred)
    rmse_test = np.sqrt(mse_test)
    mae_test = mean_absolute_error(y_test_true, y_test_pred)
    r2_test = r2_score(y_test_true, y_test_pred)

    print(f"\n----- {model_name} Performance -----")
    print("Training Performance:")
    print(f"  MSE : {mse_train:.2f}")
    print(f"  RMSE: {rmse_train:.2f}")
    print(f"  MAE : {mae_train:.2f}")
    print(f"  R²  : {r2_train:.4f}")

    print("Testing Performance:")
    print(f"  MSE : {mse_test:.2f}")
    print(f"  RMSE: {rmse_test:.2f}")
    print(f"  MAE : {mae_test:.2f}")
    print(f"  R²  : {r2_test:.4f}")

# Make predictions for both models
y_train_pred_linear = linear_model.predict(X_train_scaled)
y_test_pred_linear = linear_model.predict(X_test_scaled)

y_train_pred_rf = rf_model.predict(X_train_scaled)
y_test_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate both models
evaluate_model(y_train, y_train_pred_linear, y_test, y_test_pred_linear, "Linear Regression")
evaluate_model(y_train, y_train_pred_rf, y_test, y_test_pred_rf, "Random Forest")
